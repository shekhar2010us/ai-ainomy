{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods - formularize and calculations - Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Compute the sigmoid of x\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-1*x))    \n",
    "    return s\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Compute the gradient (derivative) of the sigmoid function with respect to its input x.\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-1*x))\n",
    "    ds = s * (1-s)    \n",
    "    return ds\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"Normalize rows of matrices\n",
    "    \"\"\"\n",
    "    x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)    \n",
    "    return x/x_norm\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "    \"\"\"\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    s = x_exp/x_sum\n",
    "    return s\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"L1 loss function\n",
    "    \"\"\"\n",
    "    loss = np.sum(np.absolute(yhat-y))\n",
    "    return loss\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"L2 loss function\n",
    "    \"\"\"\n",
    "    loss = np.sum(np.square(y-yhat))\n",
    "    return loss\n",
    "\n",
    "def log_loss(yhat, y):\n",
    "    \"\"\"log loss function\n",
    "    \"\"\"\n",
    "    loss = (-y * np.log(yhat) - (1 - y) * np.log(1 - yhat))\n",
    "    return loss\n",
    "\n",
    "def initialize(n, m):\n",
    "    \"\"\"Initialize weights and intercept\n",
    "    \"\"\"\n",
    "    W = np.random.rand(n,1)  ## init weight  \n",
    "    b = 0.5                ## init intercept\n",
    "    Z = np.zeros(m)        ## init 'z'\n",
    "    Z = Z.reshape(m,1)\n",
    "    return W, b, Z\n",
    "\n",
    "# calculate z = (W.T * x + b)\n",
    "def calc_z(W, X, b):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    return Z\n",
    "# calculate sigmoid(z)\n",
    "def calc_a(Z):\n",
    "    return sigmoid(Z)\n",
    "# calculate dL/dz\n",
    "def calc_dZ(A, Y):\n",
    "    dZ = A - Y.T\n",
    "    return dZ\n",
    "# calculate dL/dw for all 'm' samples\n",
    "def calc_dW(X, dZ, m):\n",
    "    dW = 1/m * X * dZ.T\n",
    "    return dW\n",
    "# calculate dL/db for all 'm' samples\n",
    "def calc_dB(dZ):\n",
    "    return np.sum(dZ) / m;\n",
    "# update W vector\n",
    "def update_w(W, alpha, dW):\n",
    "    new_W = W - alpha * dW\n",
    "    return new_W\n",
    "# update intercept\n",
    "def update_b(b, alpha, dB):\n",
    "    return (b - alpha * dB)\n",
    "# calculate cost\n",
    "def calc_cost(Y, A):\n",
    "    return -1 * ( np.dot(np.log(A), Y) + np.dot(np.log(1 - A), (1 - Y)) )\n",
    "\n",
    "\"\"\"logistic_regression\n",
    "X = np.matrix of training samples in columns (not rows)\n",
    "Y = np.array of labels\n",
    "alpha = learning rate\n",
    "iterations = iterations\n",
    "\"\"\"\n",
    "def logistic_regression(X, Y, alpha, iterations):\n",
    "    n = X.shape[0]    # number of features\n",
    "    m = X.shape[1]    # number of training examples\n",
    "    W, b, Z = initialize(n, m)\n",
    "    for i in range(iterations):\n",
    "        Z = calc_z(W, X, b)\n",
    "        A = calc_a(Z)\n",
    "        dZ = calc_dZ(A, Y)\n",
    "        dW = calc_dW(X, dZ, m)\n",
    "        dB = calc_dB(dZ)\n",
    "        W = update_w(W, alpha, dW)\n",
    "        b = update_b(b, alpha, dB)\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W : \n",
      " [[5.97726062]\n",
      " [6.83703893]] \n",
      "\n",
      "Final intercept: \n",
      " 0.9067971646915518 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### input\n",
    "inp = [1,2,10,12,21,23]\n",
    "y = [10,22,32]\n",
    "alpha = 0.001\n",
    "iterations = 20\n",
    "\n",
    "## reshape input and label\n",
    "x = np.reshape(np.array(inp), (m, n))\n",
    "X = np.matrix(x)      ## create matrix\n",
    "X = np.transpose(X)   ## input data matrix with records as columns\n",
    "Y = np.array(y)       ## label vector\n",
    "Y = Y.reshape(m,1)\n",
    "\n",
    "W,b = logistic_regression(X, Y, alpha, iterations)\n",
    "\n",
    "print (\"Final W : %s %s %s\" % ('\\n', W, '\\n'))\n",
    "print (\"Final intercept: %s %s %s\" % ('\\n', b, '\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
